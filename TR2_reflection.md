Mary Keenan and Sung Park
22 April 2016

Technical Review 2 Reflection


#Feedback and Decisions

The primary focus of our review was to see if there was anything we could add to our program that would make it more interesting. It seems like more than anything else, people want us to shrink the areas we’re searching -- to refine and increase our searches so that we’re looking at more specific areas (maybe states) rather than just four massive regions that make up the continental US. We hope to break up the regions we have right now so our results are more specific, but it’s hard, because Twitter’s geotag search takes in a point and a radius, so searching for tweets in an area like the east or west coast isn’t easy (because they’re long and skinny -- not circle material). We’ll try to search more than just four massive regions, but we won’t be able to do each state individually. 

Another suggestion was to add something to our program that would let us compare the results of search terms. We decided not to incorporate this into our project, because while it’s an interesting idea, we can’t think of any easy way to show the comparison on one map and if we were to make two pop up side by side, it wouldn’t be much different from running our current program twice. 

Someone else suggested we look short-term at sentiment over time. Twitter’s API won’t let you pull tweets that are more than 30 days old, so we could compare tweets from the last 15 to 30 days to tweets from the last 15 days. This would be interesting, but if we were to go down this road, we would probably want to look at tweet sentiment over time for longer than the last 30 days -- in this case, we’d need to find a database of tweets from the last x-many years. We decided not to go down the sentiment-over-time path, because looking at tweets over time for a significant period of time is a huge undertaking and we don’t want to half-ass it by only looking at sentiment over a small chunk of time. 

The secondary focus of our technical review was to figure out a better way to visualize our data -- slight differences in polarity are very hard to see and the color scheme (blue to red) is garish. All in all, the map is hard to read.

Someone suggested we create a color legend, because it’s unclear what the colors represent unless we explain it (blue is negative, red is positive). Another person suggested we overlay state borders on the choropleth map, so it’s easier for the viewer to understand which states are encompassed in which regions. 
	
Some new questions:
* Should we make our map interactive and if so, how?
* How should our program run -- should we create a web app or a program you can run from the command line?
* Should we show that we don’t cover entire regions with our tweet search (we search in a circle), and if so, how?
		

#Review Process Reflection

Overall, the review went well. We had more focused questions this time around, because we’re farther along in the project. While this helped us get the feedback we really needed instead of some less relevant suggestions, it also meant that overall we got less feedback than last time, because there was less room for people to be creative. We got answers to most of our key questions -- no one came up with a better way for us to visualize our sentiment data (we’re using a choropleth map right now), but we got suggestions on what to add to our program and how to make our maps easier to read.

A lot of the suggestions involved very simple add-ons we hadn’t thought of before, because we weren’t considering the viewer’s perspective -- we were looking at our project from a point of view where we understood what things meant and how they worked. Suggestions like including the color legend and showing state borders were valuable because we hadn’t thought about what our maps looked like to someone who didn’t know what we did (i.e. what the colors meant and which states are included in which regions). Having a different perspective proved to be very useful. 

I think we provided enough context -- people understood our project enough to be able to talk about it -- and we definitely didn’t inundate people with extraneous details. We veered a little off our agenda, because people wanted to keep testing our prototype with different words, but we followed our plan for the most part. People’s enthusiasm was encouraging, and we didn’t lose much time running extra searches. 

We were last to present again, which is amusing since in our last reflection, we said that the greatest improvement we could make would be to not go last, because people got tired and just wanted to leave. It wasn’t as bad this time -- people were more engaged, but I still think more than anything else, we could have a more effective review by not going last. People invariably get tired and the best time to present is not when people are looking at the clock or itching to pack up. I think we could also plan a more structured activity to get feedback so that even if people are tired, they’re forced to participate. 
